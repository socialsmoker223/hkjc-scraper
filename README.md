# HKJC Horse Racing Data Scraper

A Python web scraper that collects horse racing data from the Hong Kong Jockey Club (HKJC) website and stores it in a PostgreSQL database.

## Features

- Scrapes race results, runner performance, and sectional times
- **Scrapes real-time odds from HK33.com (Win/Place, Offshore markets)**
- Stores data in normalized PostgreSQL database with 9 tables
- Automatic UPSERT operations to handle duplicate data
- **Database options: Local PostgreSQL (Docker) or Supabase (Cloud)**
- Command-line interface with comprehensive summary reports
- **Enhanced logging system with dual output (console + file)**
- Tracks historical horse profile changes
- Error handling with automatic retry logic
- Modern Python tooling with `uv` package manager

## Project Structure

```
hkjc-scraper/
├── src/
│   └── hkjc_scraper/
│       ├── __init__.py          # Package exports and version
│       ├── __main__.py          # Entry point for python -m hkjc_scraper
│       ├── cli.py               # CLI interface with argparse
│       ├── config.py            # Configuration management
│       ├── database.py          # Database connection and initialization
│       ├── hk33_login.py        # HK33 requests-based login and cookie management
│       ├── hk33_scraper.py      # HK33 odds scraping (HKJC + offshore market)
│       ├── http_utils.py        # HTTP session, retry logic, rate limiting
│       ├── models.py            # SQLAlchemy ORM models (9 tables)
│       ├── persistence.py       # Data persistence layer (UPSERT operations)
│       └── scraper.py           # Web scraping functions
├── tests/
│   └── __init__.py              # Test package (no tests yet)
├── pyproject.toml               # Project metadata and dependencies (uv)
├── requirements.txt             # Pinned dependencies (generated by uv)
├── uv.lock                      # Dependency lock file
├── docker-compose.yml           # PostgreSQL + pgAdmin setup
├── Makefile                     # Common commands
├── schema.sql                   # Raw SQL schema
├── .env.example                 # Environment variables template
├── .pre-commit-config.yaml      # Pre-commit hooks configuration
├── .gitignore                   # Git ignore file
├── README.md                    # This file
├── CLAUDE.md                    # Claude Code assistant guidance
├── data_model.md                # Database schema documentation
├── ROADMAP.md                   # Development roadmap
└── HORSE_PROFILE_IMPLEMENTATION.md  # Horse profile implementation notes
```

## Requirements

- Python 3.9+
- [uv](https://github.com/astral-sh/uv) - Fast Python package installer
- Docker and Docker Compose

## Quick Start

### 1. Install uv (if not already installed)

**macOS/Linux:**
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

**Or with Homebrew:**
```bash
brew install uv
```

**Or with pip:**
```bash
pip install uv
```

### 2. Clone and Setup

```bash
cd /path/to/hkjc

# Create virtual environment with uv
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
uv pip install -e .

# Or use Makefile
make install
```

### 3. Configure Environment

```bash
# Copy example environment file
cp .env.example .env

# .env is already configured for Docker defaults - no changes needed!
```

Default `.env` values work with Docker setup:
```env
DB_HOST=localhost
DB_PORT=5432
DB_NAME=hkjc_racing
DB_USER=hkjc_user
DB_PASSWORD=hkjc_password
```

### 4. Start PostgreSQL with Docker

```bash
# Start PostgreSQL container
docker-compose up -d postgres

# Or use Makefile
make db-up

# Check it's running
docker-compose ps
```

### 5. Initialize Database

```bash
# Create tables using console script
hkjc-scraper --init-db

# Or use Makefile
make init-db
```

### 6. Scrape Data!

```bash
# Scrape races for a specific date
hkjc-scraper 2025/12/23

# Or use shorthand alias
hkjc 2025/12/23

# Or use Makefile
make scrape DATE=2025/12/23

# Or use as Python module
python -m hkjc_scraper 2025/12/23
```

## Database Options

The scraper supports two database backends:

### Option 1: Local PostgreSQL (Docker) - Default

Best for local development and full control:

```bash
# Start PostgreSQL with Docker
make db-up

# Run migrations
make migrate

# Scrape data
hkjc-scraper 2025/12/23
```

**Pros:** Fast, no internet required, full control, free
**Cons:** Requires Docker, manual backups, local-only access

### Option 2: Supabase (Cloud PostgreSQL)

Best for cloud deployment and remote access:

1. Create free Supabase account at https://supabase.com
2. Get connection string from Settings > Database
3. Configure environment:

```bash
# In .env
DATABASE_TYPE=supabase
SUPABASE_URL=postgresql://postgres.[PROJECT-REF]:[PASSWORD]@aws-0-[REGION].pooler.supabase.com:6543/postgres
```

4. Run migrations:

```bash
make migrate
```

5. Start scraping:

```bash
hkjc-scraper 2025/12/23
```

**Pros:** No Docker needed, automatic backups, remote access, managed infrastructure
**Cons:** Requires internet, free tier limits (500MB, 2GB bandwidth/month)

**See [SUPABASE_SETUP.md](SUPABASE_SETUP.md) for detailed setup guide.**

### Switching Between Databases

Change `DATABASE_TYPE` in `.env`:
- `DATABASE_TYPE=local` - Use local Docker PostgreSQL
- `DATABASE_TYPE=supabase` - Use Supabase cloud PostgreSQL

Data is isolated per database (switching doesn't migrate data).

## Usage

### Using Makefile (Recommended)

```bash
# Show all available commands
make help

# Database operations
make db-up              # Start PostgreSQL
make db-down            # Stop PostgreSQL
make db-reset           # Reset database (deletes all data)
make db-logs            # View database logs
make db-shell           # Open PostgreSQL shell
make pgadmin            # Start pgAdmin web UI

# Scraping
make scrape DATE=2025/12/23      # Scrape and save to DB
make dry-run DATE=2025/12/23     # Test without saving

# Development
make install            # Install dependencies
make dev                # Install with dev dependencies
make test               # Run tests
make lint               # Run linter
make format             # Format code
make clean              # Clean temporary files
```

### Using Console Scripts

```bash
# Scrape races for a specific date
hkjc-scraper 2025/12/23

# Initialize database tables
hkjc-scraper --init-db

# Dry run (test without saving)
hkjc-scraper 2025/12/23 --dry-run

# Scrape with horse profiles
hkjc-scraper 2025/12/23 --scrape-profiles

# Date range scraping
hkjc-scraper --date-range 2025/12/01 2025/12/31

# Backfill historical data
hkjc-scraper --backfill 2024/01/01 2024/12/31

# Update mode (scrape from last DB entry to today)
hkjc-scraper --update

# Force re-scrape existing data
hkjc-scraper 2025/12/23 --force

# Or use as Python module
python -m hkjc_scraper 2025/12/23
```

### Command-Line Options

```
hkjc-scraper [-h] [DATE | --init-db | --date-range START END | --backfill START END | --update]
             [--dry-run] [--force] [--scrape-profiles]

Positional Arguments:
  DATE                    Race date in YYYY/MM/DD format (e.g., 2025/12/23)

Mode Selection (mutually exclusive):
  --init-db               Initialize database tables only
  --date-range START END  Scrape a range of dates (YYYY/MM/DD)
  --backfill START END    Backfill mode: scrape range of dates
  --update                Update mode: scrape from last DB entry to today

Optional Flags:
  -h, --help              Show help message
  --dry-run               Scrape data but don't save to database
  --force                 Force re-scrape even if data exists
  --scrape-profiles       Also scrape horse profiles from Horse.aspx

HK33 Odds Scraping (Requires .hk33_cookies):
  --scrape-hk33           Scrape both HKJC odds AND offshore market
  --scrape-hk33-odds      Scrape ONLY HKJC Win/Place odds
  --scrape-hk33-market    Scrape ONLY offshore market data
  --login-hk33            Auto-login via requests to refresh cookies
```

## Database Management

### PostgreSQL (Docker)

```bash
# Start database
docker-compose up -d postgres

# Stop database
docker-compose down

# View logs
docker-compose logs -f postgres

# Access PostgreSQL shell
docker-compose exec postgres psql -U hkjc_user -d hkjc_racing

# Reset database (WARNING: deletes all data)
docker-compose down -v
docker-compose up -d postgres
python database.py
```

### pgAdmin (Web UI)

pgAdmin provides a graphical interface to browse and query the database.

```bash
# Start pgAdmin (optional)
docker-compose --profile tools up -d pgadmin

# Or use Makefile
make pgadmin
```

Then open http://localhost:5050 in your browser:
- **Email:** admin@hkjc.local
- **Password:** admin

**Add Server in pgAdmin:**
1. Right-click "Servers" → "Create" → "Server"
2. Name: `HKJC Racing`
3. Connection tab:
   - Host: `postgres` (Docker network name)
   - Port: `5432`
   - Database: `hkjc_racing`
   - Username: `hkjc_user`
   - Password: `hkjc_password`

## Logging & Monitoring

### Summary Reports

Every scraping run displays a comprehensive summary report:

```
============================================================
SCRAPING SUMMARY
============================================================
Duration: 45.2s (0.8 minutes)

Date Statistics:
  Total dates processed: 7
  Successfully scraped:  6
  Skipped (existing):    1
  Failed (errors):       0
  Success rate:          100.0%

Data Statistics:
  Races scraped:         60
  Runners saved:         720
  Sectionals saved:      7200
  Profiles saved:        180

Error Breakdown:
  Network errors:        0
  Parse errors:          0
  Database errors:       0
  Other errors:          0
  Total errors:          0
============================================================
```

### Log Files

All operations are logged to `hkjc_scraper.log`:

```bash
# View recent logs
tail -f hkjc_scraper.log

# Search for errors
grep ERROR hkjc_scraper.log
```

**Log Format:**
```
2026-01-06 14:30:45 - hkjc_scraper.cli - INFO - Scraping completed: 6/7 successful
2026-01-06 14:30:45 - hkjc_scraper.scraper - ERROR - Parse error for 2025/12/23: Missing field
```

### Log Level Control

Control logging verbosity via environment variable:

```bash
# Normal operation (default)
LOG_LEVEL=INFO hkjc-scraper 2025/12/23

# Verbose debugging
LOG_LEVEL=DEBUG hkjc-scraper 2025/12/23

# Errors only
LOG_LEVEL=ERROR hkjc-scraper 2025/12/23
```

**Configuration in .env:**
```env
LOG_LEVEL=INFO              # DEBUG, INFO, WARNING, ERROR
LOG_FILE=hkjc_scraper.log   # Path to log file
```

**HK33 Scraping Configuration (Optional):**
```env
HK33_EMAIL=your_email@example.com
HK33_PASSWORD=your_password
HK33_REQUEST_TIMEOUT=30
RATE_LIMIT_HK33_SAME_PATH=0.3       # Delay for same endpoint (seconds)
RATE_LIMIT_HK33_PATH_CHANGE=15.0    # Delay when switching bet types (seconds)
MAX_HK33_RACE_WORKERS=2             # Concurrent race scraping per bet type
HK33_MAX_RELOGINS=3                 # Max auto re-login attempts per session
```

**Console vs File Output:**
- **Console**: Clean, user-friendly messages (INFO and above)
- **File**: Detailed logs with timestamps (DEBUG and above)

## Database Schema

The database consists of 9 tables:

### Core Tables
- **meeting** - Race meetings (date, venue)
- **race** - Individual races (class, distance, track info)
- **horse** - Horse master data
- **jockey** - Jockey master data
- **trainer** - Trainer master data

### Performance Tables
- **runner** - Per-race, per-horse performance
- **horse_sectional** - Sectional time details

### Profile Tables
- **horse_profile** - Current horse profile snapshot
- **horse_profile_history** - Historical profile tracking

### Odds Tables
- **hkjc_odds** - Time-series HKJC odds data
- **offshore_market** - Offshore market data (Bet/Eat prices)

See `data_model.md` for detailed schema documentation.

## What Gets Scraped

### From LocalResults.aspx (Race Results)
- Meeting information (date, venue)
- Race details (name, class, distance, track, going, prize)
- Runner performance (finish position, weights, draw, times, odds)
- Horse, jockey, trainer information

### From DisplaySectionalTime.aspx (Sectional Times)
- Per-section performance for each horse
- Section positions and margins
- Split times for each section

### From Horse.aspx (Horse Profiles) ✅ **Implemented**
Use `--scrape-profiles` flag to enable:
- Horse profiles (origin, age, colour, sex, import type, current location)
- Career statistics (season/lifetime prize money, wins, seconds, thirds, starts)
- Rating information (current rating, season start rating)
- Owner and pedigree information (owner name, sire, dam, dam sire)
- Import details (import date, import type)
- Historical profile tracking in `horse_profile_history` table

See `HORSE_PROFILE_IMPLEMENTATION.md` for implementation details.

### From HK33.com (Odds) ✅ **Implemented**
Use `--scrape-hk33` flag to enable:
- HKJC Win/Place odds (time-series)
- Offshore Market odds (Bet/Eat prices) for arbitrage analysis
- Auto-login via requests (set `HK33_EMAIL` and `HK33_PASSWORD` in `.env`)
- Automatic session recovery on 403 errors with re-login
- Adaptive rate limiting with by-type scraping strategy
- User-Agent rotation and request jitter for anti-bot evasion

## Development

### Install Dev Dependencies

```bash
# With uv
uv pip install -e ".[dev]"

# Or with Makefile
make dev
```

Dev dependencies include:
- `pytest` - Testing framework
- `pytest-cov` - Test coverage
- `mypy` - Type checking
- `ruff` - Fast linter and formatter
- `ipython` - Enhanced Python shell

### Code Quality

```bash
# Format code
make format

# Run linter
make lint

# Run tests (when implemented)
make test

# Type checking
uv run mypy .
```

### Project Structure Best Practices

- HKJC scraping logic in `scraper.py`
- HK33 odds scraping in `hk33_scraper.py`
- HK33 login/cookies in `hk33_login.py`
- Database operations in `persistence.py`
- ORM models in `models.py`
- CLI interface in `cli.py`
- Configuration in `config.py`

## Troubleshooting

### Docker Issues

**Error: `Cannot connect to the Docker daemon`**
- Make sure Docker Desktop is running
- Check: `docker ps`

**Error: `port is already allocated`**
- Another service is using port 5432
- Change port in `.env`: `DB_PORT=5433`
- Update `docker-compose.yml` ports: `5433:5432`

### Database Connection Errors

**Error: `could not connect to server`**
- Check Docker container is running: `docker-compose ps`
- Check logs: `docker-compose logs postgres`
- Verify credentials in `.env` match `docker-compose.yml`

**Error: `database "hkjc_racing" does not exist`**
- Run: `make db-reset` to recreate database
- Or manually: `python database.py`

### Scraping Errors

**Error: `No races found`**
- Verify the date has races scheduled on HKJC website
- Check date format is correct (YYYY/MM/DD)

**Error: `HTTP 404` or `Timeout`**
- HKJC website structure may have changed
- Check internet connection
- Try again later (site may be down)

### uv Issues

**Error: `uv: command not found`**
- Install uv: `curl -LsSf https://astral.sh/uv/install.sh | sh`
- Or: `brew install uv` (macOS)

**Dependencies not resolving**
- Clear cache: `uv cache clean`
- Reinstall: `uv pip install -e . --reinstall`

### HK33 Scraping Errors

**Error: `403 Forbidden` from HK33.com**
- Cookies expired or invalid
- Session expired: scraper auto-recovers with re-login (up to `HK33_MAX_RELOGINS` attempts)
- Manual fix: Run `hkjc-scraper --login-hk33` to refresh cookies

**Error: `Missing .hk33_cookies file`**
- Cookie file not found
- Solution: Set `HK33_EMAIL` and `HK33_PASSWORD` in `.env`, then run `hkjc-scraper --login-hk33`

**HK33 data not appearing in database**
- Check if --scrape-hk33 flag was used
- Verify cookies are valid (check HK33 website manually)
- Check logs: `grep HK33 hkjc_scraper.log`

## Development Status

**Current Phase:** Phase 5 - Quality & Testing
**Overall Completion:** ~98%

### Completed Phases

**Phase 1: Core Infrastructure** ✅ 100% COMPLETED
- ✅ Database setup with SQLAlchemy ORM (9 tables)
- ✅ Database migrations with Alembic (alembic/ directory)
- ✅ Configuration management (.env, config.py)
- ✅ Data persistence layer with UPSERT operations
- ✅ Command-line interface with argparse
- ✅ Docker setup with PostgreSQL + pgAdmin
- ✅ Modern tooling (uv, Makefile, ruff, mypy, pre-commit)

**Phase 2: Complete Scraping** ✅ 100% COMPLETED
- ✅ Race results scraping (LocalResults.aspx)
- ✅ Sectional time scraping (DisplaySectionalTime.aspx)
- ✅ Horse profile scraping (Horse.aspx)
- ✅ Incremental updates (--date-range, --backfill, --update modes)

**Phase 3: Production Hardening** ✅ 98% COMPLETED
- ✅ Comprehensive error handling with retry logic
- ✅ Enhanced logging system with dual output (console + file)
- ✅ Summary reports with detailed statistics
- ✅ Rate limiting for HTTP requests
- ✅ Supabase cloud database integration
- ✅ Incremental updates and smart scraping

**Phase 4: Usability & Automation** ⏳ 85% IN PROGRESS
- ✅ Full-featured CLI with all modes
- ✅ Console scripts (hkjc-scraper, hkjc)
- ✅ Progress bars for multi-date operations
- ❌ Scheduling/automation (cron, APScheduler, systemd)

**Phase 5: Quality & Testing** ⏳ 30% IN PROGRESS
- ✅ Tooling configured: pytest, mypy, ruff, pre-commit
- ❌ Pytest test suite for scraping functions (TOP PRIORITY)

### Current Focus
- Write comprehensive pytest test suite for scraping functions
- Add scheduling/automation examples

### Quick Start for Contributors
```bash
# Install with dev dependencies
make dev

# Format and lint
make format
make lint

# Run tests (when implemented)
make test
```

See `ROADMAP.md` for the complete development plan and feature roadmap.

## Notes

- The scraper respects HKJC's website structure as of December 2024
- Data is stored in Traditional Chinese (as presented on HKJC site)
- UPSERT logic prevents duplicate data on re-scraping
- Only scrapes publicly available data
- Docker volumes persist data between container restarts

## License

This project is for educational and personal use only. Please respect HKJC's terms of service and robots.txt.

## Contributing

See `ROADMAP.md` for planned features and improvements.
